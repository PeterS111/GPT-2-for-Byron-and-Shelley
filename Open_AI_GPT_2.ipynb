{
   "cells": [
    
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT_2_CODE/blob/main/Open_AI_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZCyyVBobiKx"
   },
   "source": [
    "# Open_AI_GPT-2\n",
    "version 10.06.2022\n",
    "\n",
    "The code is based on https://github.com/nshepperd/gpt-2 by N Shepperd (which, in turn, is based on the original OpenAI code), with some changes by Peter S.  \n",
    "\n",
    "This notebook contains the code which was used in our paper \"Training GPT-2 to represent two Romantic-era authors challenges, evaluations and pitfalls\" to fine-tune and generate text from OpenAI GPT-2 models. \n",
    "\n",
    "GPUs on Colab only allow for fine-tuning of the Small (124M) and Medium (345M) models.\n",
    "\n",
    "Make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\". \n",
    "\n",
    "First, check that your runtime is using a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvW0T8QPFIJ5"
   },
   "outputs": [],
   "source": [
    "# check the GPU:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjVyrwPXFOY_"
   },
   "source": [
    "## 1. Download the repository from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awo9HQFoFR7o"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/PeterS111/GPT_2_CODE/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J5l-C-1Ho6f"
   },
   "source": [
    "## 2. Select TensorFlow version\n",
    "Run the following command to ensure you have TensorFlow v 1.x. This code doesn't work with versions 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRQMSI9GHruW"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8t-Wi3Jhxpx"
   },
   "source": [
    "## 3. Change the working directory to the main folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkuBaaCoTo99"
   },
   "outputs": [],
   "source": [
    "cd /content/GPT_2_CODE/Open_AI_GPT_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oe0SBj9SiPOJ"
   },
   "source": [
    "## 4. Install the requirements\n",
    "After installation you will get the message that \"You must restart the runtime in order to use newly installed versions.\" ⚡**DON'T DO IT!**⚡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpwYj-rsHyhO"
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rtqz40GitFB"
   },
   "source": [
    "## 5. Create the 'models' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AWnQjZrixs5"
   },
   "outputs": [],
   "source": [
    "!mkdir models/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m45FxuYlJd49"
   },
   "source": [
    "## 6. Download the pre-trained GPT-2 model\n",
    "The command below will download the GPT-2 Small: 124M parameters. If you want GPT-2 Medium version, change the parameter to 345M. Fine-tuning the Large (774M) and XLarge (1558M) is not possible on Colab (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPe-3wjDMKSK"
   },
   "outputs": [],
   "source": [
    "!python download_model.py 124M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi4bFs2-NZlx"
   },
   "source": [
    "## 7. Batch fine-tune models and generate samples\n",
    "\n",
    "To run batch fine-tuning and generation go to \"run_all.py\" and edit accordingly. Uncomment the line below and run it. Please be aware that running that script with its original settings will exceed the maximum running time that Colab allows.\n",
    "\n",
    "If you want to experiment with a single model and smaller number of samples, please follow form step 8.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNjbjYBgUSVe"
   },
   "outputs": [],
   "source": [
    "#!python run_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLpIp69Gx3Zf"
   },
   "source": [
    "### 7.1. Zip the outputs for download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBsSdD30yDY_"
   },
   "outputs": [],
   "source": [
    "#!zip \"outputs.zip\" outputs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqjRXMNgdRQ5"
   },
   "source": [
    "## 8. Fine-tuning the model\n",
    "\n",
    "You can fine tune the models with or without validation dataset. Uncomment as appropriate.\n",
    "Make sure that \"--model_steps\" and \"--save_every\" parameters for \"train.py\" script and the \"--model_steps\" for \"mover.py\" are all the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKjMtjVZfDwO"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning the model without validatation:\n",
    "!python train.py --dataset \"input_data/Shelley.txt\" --model_name 124M --max_steps 10000 --save_every 10000\n",
    "    \n",
    "# Fine-tuning the model with validatation:\n",
    "# !python train.py --dataset \"input_data/Shelley_train.txt\" --val_dataset \"input_data/Shelley_val.txt\" --val_every 50 --model_name 124M --max_steps 10000 --save_every 10000\n",
    " \n",
    "# This line calls the \"mover.py\" script to copy the fine-tuned model into the \"models\" folder\n",
    "# and to copy from the original pre-trained model the files required to run the fine-tuned model:\n",
    "!python mover.py --model_steps 10000 --model_size 124M --directory_path \"/content/GPT_2_CODE/Open_AI_GPT_2/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Rwjy9LGgJX6"
   },
   "source": [
    "## 9 (OPTIONAL) Export the fine-tuned model to Google Drive\n",
    "If you want to save the fine-tuned model for later use, follow these steps:\n",
    "\n",
    "### 9.1. Zip the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7q7d9BWJlITC"
   },
   "outputs": [],
   "source": [
    "!tar -czvf \"my_model_10000_steps.tar.gz\" models/model_10000/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj245cUnlj67"
   },
   "source": [
    "### 9.2. Mount Google Drive (it will require authentication):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZjbBymzlcB5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9ZlS3CelymS"
   },
   "source": [
    "### 9.3. Export the compressed model to Drive. This only takes few minutes, but sometimes you may have to run the command twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jt4tnp88lqo6"
   },
   "outputs": [],
   "source": [
    "!cp \"my_model_10000_steps.tar.gz\" \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2-QNn4mXDz"
   },
   "source": [
    "## 10 (OPTIONAL) Importing and running the saved model from Google Drive\n",
    "If you want to generate samples from the saved model (after you have closed this notebook), do the following:\n",
    "\n",
    "Run step 1. Download the repository from github\n",
    "\n",
    "Run step 2. Select TensorFlow version\n",
    "\n",
    "Run step 3. Change the working directory to the main folder\n",
    "\n",
    "Run step 4. Install the requirements\n",
    "\n",
    "Run step 5. Create the 'models' folder\n",
    "\n",
    "After you done that: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqHsYU-MoYU4"
   },
   "source": [
    "### 10.1. Mount Google Drive, the same as 9.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYDTmbJZnw1x"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r8-Il77oneJ"
   },
   "source": [
    "### 10.2. Import the previously saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt1_1lI6of7H"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/my_model_10000_steps.tar.gz\" \"/content/GPT_2_CODE/Open_AI_GPT_2/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3UrZOoEpEBo"
   },
   "source": [
    "### 10.3. Unpack the model and remove the .tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zqxnGaspGLo"
   },
   "outputs": [],
   "source": [
    "!tar xf /content/GPT_2_CODE/Open_AI_GPT_2/models/my_model_10000_steps.tar.gz\n",
    "!rm -v /content/GPT_2_CODE/Open_AI_GPT_2/models/my_model_10000_steps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUShfULkpedg"
   },
   "source": [
    "## 11. Generate samples\n",
    "\n",
    "Make sure that \"--model_name\" parameter matches the models you trained in point 8.\n",
    "For example, if your model was trained for 10000 steps the parameter should be \"model_10000\".\n",
    "\n",
    "\"seed_start\" and \"seed_end\" variables set the range of samples being generated from your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kivsRBO0pTOY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "seed_start = 1\n",
    "seed_end = 11\n",
    "\n",
    "prompt = \"The eternal sky \"\n",
    "model_descr = \"Shelley\"\n",
    "model_name = \"model_10000\"\n",
    "length = 1000\n",
    "\n",
    "prompt = prompt.replace(\" \", \"£££££\")\n",
    "\n",
    "for s in range(seed_start, seed_end):\n",
    "    os.system('python generate_conditional_samples_to_file.py --raw_text {prompt} --model_descr {model_descr} --model_name {model_name} --length {length} --seed {s} --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 1'.format(prompt=prompt, model_descr=model_descr, model_name=model_name, length=length, s=s))\n",
    "    print(\"Generating sample with seed: \" + str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVGK79oZgT5k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Open_AI_GPT-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
