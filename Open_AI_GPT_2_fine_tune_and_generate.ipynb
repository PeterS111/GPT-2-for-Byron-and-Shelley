{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Open_AI_GPT-2_fine-tune_and_generate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT-2-for-Byron-and-Shelley/blob/main/Open_AI_GPT_2_fine_tune_and_generate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCyyVBobiKx"
      },
      "source": [
        "# Open_AI_GPT-2_fine-tune_and_generate\n",
        "version 18.05.2022\n",
        "\n",
        "The code is based on https://github.com/nshepperd/gpt-2 by N Shepperd (which, in turn, is based on the original OpenAI code), with some changes by Peter S.  \n",
        "\n",
        "This notebook contains the code which was used in our paper \"Training GPT-2 to represent two Romantic-era authors challenges, evaluations and pitfalls\" to fine-tune and generate text from OpenAI GPT-2 models. \n",
        "\n",
        "GPUs on Colab only allow for fine-tuning of the Small (124M) and Medium (345M) models.\n",
        "\n",
        "Make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\". \n",
        "\n",
        "First, check that your runtime is using a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the GPU:\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "AvW0T8QPFIJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FUW8jxSvtxXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download the repository from github:"
      ],
      "metadata": {
        "id": "cjVyrwPXFOY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PeterS111/Open_AI_GPT_2_fine_tune_and_generate/"
      ],
      "metadata": {
        "id": "Awo9HQFoFR7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Select TensorFlow version\n",
        "Run the following command to ensure you have TensorFlow v 1.x. This code doesn't work with versions 2.x"
      ],
      "metadata": {
        "id": "9J5l-C-1Ho6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "yRQMSI9GHruW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install the requirements\n",
        "After installation you will get the message that \"You must restart the runtime in order to use newly installed versions.\" ⚡**DON'T DO IT!**⚡"
      ],
      "metadata": {
        "id": "mzpC3eRfHwuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r /content/Open_AI_GPT_2_fine_tune_and_generate/requirements.txt "
      ],
      "metadata": {
        "id": "EpwYj-rsHyhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Download the pre-trained GPT-2 model\n",
        "The command below will download the GPT-2 Small: 124M parameters. If you want GPT-2 Medium version, change the parameter to 345M. Fine-tuning the Large (774M) and XLarge (1558M) is not possible on Colab (for now)."
      ],
      "metadata": {
        "id": "m45FxuYlJd49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Open_AI_GPT_2_fine_tune_and_generate/download_model.py 124M"
      ],
      "metadata": {
        "id": "Qs_j389WJfod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Move the downloaded model to the main folder"
      ],
      "metadata": {
        "id": "tVIovlL1KPnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -arv  /content/models/ /content/Open_AI_GPT_2_fine_tune_and_generate/models/\n",
        "!rm -r /content/models/"
      ],
      "metadata": {
        "id": "mUYxK5uRKRF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Change the working directory to the main folder"
      ],
      "metadata": {
        "id": "wU9aLFuDKta8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/Open_AI_GPT_2_fine_tune_and_generate/"
      ],
      "metadata": {
        "id": "sZ5sgEMGKzqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Batch fine-tune models and generate samples\n",
        "\n",
        "To run batch fine-tuning and generation go to \"run_all.py\" and edit accordingly. Uncomment the line below and run it. Please be aware that running that script with its original settings will exceed the maximum running time that Colab allows.\n",
        "\n",
        "If you want to experiment with a single model and smaller number of samples, please follow the steps below.\n",
        " "
      ],
      "metadata": {
        "id": "hi4bFs2-NZlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python run_all.py"
      ],
      "metadata": {
        "id": "jNjbjYBgUSVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Fine-tuning the model\n"
      ],
      "metadata": {
        "id": "TqjRXMNgdRQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning the model without validatation:\n",
        "#!python train.py --dataset \"input_data/Shelley.txt\" --model_name 124M --top_k 50 --top_p 1.0 --max_steps 1000 --save_every 1000 --sample_every 100000 --max_to_keep 10  --val_batch_count 40\n",
        "    \n",
        "# Fine-tuning the model with validatation:\n",
        "!python train.py --dataset \"input_data/Shelley_train.txt\" --val_dataset \"input_data/Shelley_train.txt\" --val_every 50 --model_name 124M --top_k 50 --top_p 1.0 --max_steps 1000 --save_every 1000 --max_to_keep 10  --val_batch_count 40\n",
        " \n",
        "# This line calls the \"mover.py\" script to copy the fine-tuned model into the \"models\" folder\n",
        "# and to copy from the original pre-trained model the files required to run the fine-tuned model:\n",
        "!python mover.py --model_steps 1000 --model_size 124M --directory_path \"/content/Open_AI_GPT_2_fine_tune_and_generate/\" "
      ],
      "metadata": {
        "id": "UKjMtjVZfDwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 (OPTIONAL) Export the fine-tuned model to Google Drive\n",
        "If you want to save the fine-tuned model for later use, follow these steps:\n",
        "\n",
        "### 9.1. Zip the model."
      ],
      "metadata": {
        "id": "_Rwjy9LGgJX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf \"my_model_1000_steps.tar.gz\" models/model_1000/*"
      ],
      "metadata": {
        "id": "7q7d9BWJlITC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2. Mount Google Drive (it will require authentication):"
      ],
      "metadata": {
        "id": "Wj245cUnlj67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yZjbBymzlcB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3. Export the compressed model to Drive. This only takes few minutes, but sometimes you may have to run the command twice."
      ],
      "metadata": {
        "id": "z9ZlS3CelymS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"my_model_1000_steps.tar.gz\" \"/content/drive/My Drive/\""
      ],
      "metadata": {
        "id": "jt4tnp88lqo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 (OPTIONAL) Importing and running the saved model from Google Drive\n",
        "If you want to generate samples from the saved model (after you have closed this notebook), do the following:\n",
        "\n",
        "Run step 1. Download the repository from github\n",
        "\n",
        "Run step 2. Select TensorFlow version\n",
        "\n",
        "Run step 3. Install the requirements\n",
        "\n",
        "Run step 6. Change the working directory to the main folder\n",
        "\n",
        "After you done that: \n",
        "\n"
      ],
      "metadata": {
        "id": "zq2-QNn4mXDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.1 Create the \"models\" folder:"
      ],
      "metadata": {
        "id": "jwmoTvqZoQNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models/ "
      ],
      "metadata": {
        "id": "u4VK25akl3QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2. Mount Google Drive, the same as 9.2:"
      ],
      "metadata": {
        "id": "SqHsYU-MoYU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lYDTmbJZnw1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3. Import the previously saved model:"
      ],
      "metadata": {
        "id": "9r8-Il77oneJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/my_model_1000_steps.tar.gz\" \"/content/Open_AI_GPT_2_fine_tune_and_generate/models/\""
      ],
      "metadata": {
        "id": "pt1_1lI6of7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4. Unpack the model and remove the .tar file:"
      ],
      "metadata": {
        "id": "S3UrZOoEpEBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf /content/Open_AI_GPT_2_fine_tune_and_generate/models/my_model_1000_steps.tar.gz\n",
        "!rm -v /content/Open_AI_GPT_2_fine_tune_and_generate/models/my_model_1000_steps.tar.gz"
      ],
      "metadata": {
        "id": "0zqxnGaspGLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Generate samples"
      ],
      "metadata": {
        "id": "iUShfULkpedg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# seed_start and seed_end variable set the range of samples being generated from your model:\n",
        "\n",
        "seed_start = 1\n",
        "seed_end = 11\n",
        "for s in range(seed_start, seed_end):\n",
        "    os.system('python generate_conditional_samples_to_file.py --raw_text \"The eternal sky \" --model_descr \"Shelley\" --model_name model_1000 --length 600 --seed {} --temperature 1.0 --top_k 50 --top_p 1.0 --nsamples 1'.format(s))\n",
        "    print(\"Generating sample with seed: \" + str(s))\n"
      ],
      "metadata": {
        "id": "kivsRBO0pTOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QMtzcgmPrSnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
