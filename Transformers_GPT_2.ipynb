{
   "cells": [
    
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterS111/GPT_2_CODE/blob/main/Transformers_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAfruuXn1RJE"
   },
   "source": [
    "\n",
    "# Transformers_GPT_2\n",
    "version 10.06.2022\n",
    "\n",
    "The code is based on https://github.com/priya-dwivedi/Deep-Learning/ by Priyanka Dwivedi, with small changes by Peter S\n",
    "\n",
    "This notebook contains the code which was used in our paper \"Training GPT-2 to represent two Romantic-era authors challenges, evaluations and pitfalls\" to fine-tune and generate text from OpenAI GPT-2 models.\n",
    "\n",
    "Base GPUs on Colab only allow for fine-tuning of the Small (124M) and Medium (345M) models.\n",
    "\n",
    "Make sure that you are using a GPU: Runtime/Change runtime type/ -> Select \"GPU\".\n",
    "\n",
    "First, check that your runtime is using a GPU: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfnKQwQQ1_gk",
    "outputId": "fcc84576-01b6-4eec-c0de-0911077de97b"
   },
   "outputs": [],
   "source": [
    "# check the GPU:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98IxMgje2HZT"
   },
   "source": [
    "## 1. Download the repository from github:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3Qv6nfmrXPr",
    "outputId": "0cd3490f-3533-4226-f54b-a9e3b7916319"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/PeterS111/GPT_2_CODE/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLC20YbW2NcD"
   },
   "source": [
    "## 2. Change the working directory to the main folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_xfH-FJrXne",
    "outputId": "c0f29893-300f-441b-edf5-db0deae8674e"
   },
   "outputs": [],
   "source": [
    "cd /content/GPT_2_CODE/Transformers_GPT_2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWcBZeOR2jsb"
   },
   "source": [
    "## 3. Install Tranformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07S832Y0rX1y",
    "outputId": "b5bd921e-7264-4995-a61e-a723c4d3a964"
   },
   "outputs": [],
   "source": [
    "pip install \"transformers==2.7.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DL6au0g7F_v"
   },
   "source": [
    "## 4. Batch fine-tune models and generate samples\n",
    "To run batch fine-tuning and generation go to \"run_all.py\" and edit accordingly. Uncomment the line below and run it.\n",
    "\n",
    "If you want to experiment with a single model and smaller number of samples, please follow the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPO-b7l97Pue",
    "outputId": "c680d6ff-d789-4aa8-c28c-005f5cab4b6c"
   },
   "outputs": [],
   "source": [
    "!python run_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLd4egEA2iUz"
   },
   "source": [
    "## 5. Fine_tuning the model\n",
    "\n",
    "Datasets of Byron's novels and Shelley's collected works are provided with the notebook. You can of course replace those datasets with your own. In that case you will have to upload them to /content/GPT_2_CODE/Transformers_GPT_2/input_data. You can \"drag and drop\" them from your PC. You will have to change the argument: \"--train_data_file\" accordingly.\n",
    "\n",
    "To control the training time you can change the number of training steps.\n",
    "\n",
    "To train the Medium model change the following argument:\n",
    "\n",
    "--model_name_or_path=gpt2\n",
    "\n",
    "to:\n",
    "\n",
    "--model_name_or_path=gpt2-medium\n",
    "\n",
    "Make sure that \"--max_steps\" and \"--save_steps\" arguments have the same value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryAY2670rYEJ"
   },
   "outputs": [],
   "source": [
    "!python run_lm_finetuning.py --output_dir=output --model_name_or_path=gpt2 --do_train --train_data_file=input_data/Shelley.txt --overwrite_output_dir --max_steps 10000 --save_steps 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwCHsizsNDEU"
   },
   "source": [
    "## 6 (OPTIONAL) Exporting the fine-tuned model to Google Drive\n",
    "\n",
    "If you want to save the fine-tuned model for later use, follow these steps:\n",
    "\n",
    "###6.1. Zip the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psapBGyyvCVs"
   },
   "outputs": [],
   "source": [
    "!tar -czvf \"my_model_10000_steps.tar.gz\" output/checkpoint-10000/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjhvwImaNoQb"
   },
   "source": [
    "###6.2. Mount Google Drive (it will require authentication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjG5YkiNx1jx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKCvk7FuNzKC"
   },
   "source": [
    "###6.3. Export the compressed model to Drive. This only takes few minutes, but sometimes you may have to run the command twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBjg7yXrPJri"
   },
   "outputs": [],
   "source": [
    "!cp \"my_model_10000_steps.tar.gz\" \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuM51wb7OG47"
   },
   "source": [
    "##7 (OPTIONAL) Importing saved model from Google Drive\n",
    "\n",
    "If you want to return to saved model, do the following:\n",
    "\n",
    "Run step 1. Download the repository from github\n",
    "\n",
    "Run step 2. Change the working directory to the main folder\n",
    "\n",
    "Run step 3. Install Tranformers library\n",
    "\n",
    "the same as above. Then:\n",
    "\n",
    "###7.1. Create the output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9xw7VNmvDGH"
   },
   "outputs": [],
   "source": [
    "!mkdir output/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPCGLRuoPdkC"
   },
   "source": [
    "###7.2. Mount Google Drive, the same as 6.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v0d02YBPk3q"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOcpDErrPoAD"
   },
   "source": [
    "###7.3. Import the previously saved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrF-v_SZ232k"
   },
   "outputs": [],
   "source": [
    "!cp \"/content/drive/My Drive/my_model_10000_steps.tar.gz\" /content/GPT_2_CODE/Transformers_GPT_2/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF_kVCptPuGy"
   },
   "source": [
    "###7.4. Unpack the model and remove the .tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xyKdKbzvDUT"
   },
   "outputs": [],
   "source": [
    "!tar xf /content/GPT_2_CODE/Transformers_GPT_2/output/my_model_10000_steps.tar.gz\n",
    "!rm -v /content/GPT_2_CODE/Transformers_GPT_2/output/my_model_10000_steps.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4xxxQKjQL0c"
   },
   "source": [
    "## 8. Generate text:\n",
    "\n",
    "First check the folder \"content/GPT_2_CODE/Transformers_GPT_2/\"output for a folder named \"checkpoint- *\". In our case the folder is \"checkpoint-10000\". If you changed the training time, the folder may be different. Make sure that the parameter \"--model_name_or_path output\" has the correct argument. You can change the \"--prompt\" parameter to any string. To generate another sample with a different seed, change the \"--seed\" parameter to any positive integer.\n",
    "\n",
    "Generated text will appear in the output cell and a copy will be saved in the \"/GPT_2_CODE/Transformers_GPT_2/outputs\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBgJnJB5vDj_"
   },
   "outputs": [],
   "source": [
    "!python generate_conditional_samples_to_file.py --model_type gpt2 --model_name_or_path output/checkpoint-10000 --length 500 --prompt \"The blue sky\" --seed 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64TT_JxIdqJo"
   },
   "source": [
    "###8.1. Zip the outputs folder for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39q1glJKdyKm"
   },
   "outputs": [],
   "source": [
    "!zip \"outputs.zip\" outputs/*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Copy of Transformers_GPT_2_fine_tune_and_generate .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
